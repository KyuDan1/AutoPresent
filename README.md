<div align="center">
  <h1>AutoPresent: Designing Structured Visuals From Scratch</h1>
</div>

This is the code for the paper [**AutoPresent: Designing Structured Visuals From Scratch**](https://www.arxiv.org/abs/2501.00912) by [Jiaxin Ge*](https://jiaxin.ge/), [Zora Zhiruo Wang*](https://zorazrw.github.io/), [Xuhui Zhou](https://xuhuiz.com/), [Yi-Hao Peng](https://www.yihaopeng.tw/), [Sanjay Subramanian](https://people.eecs.berkeley.edu/~sanjayss/), [Qinyue Tan](https://openreview.net/profile?id=~Qinyue_Tan1), [Maarten Sap](https://maartensap.com/), [Alane Suhr](https://www.alanesuhr.com/), [Daniel Fried](https://dpfried.github.io/), [Graham Neubig](https://www.phontron.com/), [Trevor Darrell](https://people.eecs.berkeley.edu/~trevor/).

## Quick Start

### ðŸŒŸ Try Our online Huggingface Demo!  

[**Click here**](https://huggingface.co/spaces/JiaxinGe/AutoPresent) to try our demo on Huggingface.  

## Contents
This repository contains the code for AutoPresent.

- [Install](#install)
- [SlidesBench](#slidesbench)
- [Create Slides via Programs](#create-slides-via-programs)
  - [Generate Programs](#generate-programs)
  - [Refinement](#refinement)
  - [Execution](#execution)
- [AutoPresent: Training Slide Generation Models](#autopresent-training-slide-generation-models)
  - [Model Weights](#model-weights)
  - [Model Training](#model-training)
- [Evaluation](#evaluation)
  - [Reference-based Evaluation](#reference-based-evaluation)
  - [Reference-free Evaluation](#reference-free-evaluation)

## Install

```bash
conda create -n slides python=3.10
conda activate slides

pip install -r requirements.txt
pip insatll . # to install SlidesLib
sudo apt-get install unoconv  # for converting pptx to jpg/pdf
```

You need to have an OpenAI API key to run the code. We recommend:

```bash
conda env config vars set OPENAI_API_KEY=your_key
```

## SlidesBench

The `slidesbench` directory includes code for:

1. creating examples for any given slide deck in PPTX format, and
2. auto-generate reference python code to create the slides.

See `slidesbench/README.md` for more details.


## Create Slides via Programs

### Generate Programs

```bash
cd generate
```

To generate programs to create slide for the entire slide deck:

```bash
# sufficient instruction setting
python create_slide_deck.py --slide_deck "art_photos" --setting "perfect"

# visual absence setting
python create_slide_deck.py --slide_deck "art_photos" --setting "visual"

# creative generation setting
python create_slide_deck.py --slide_deck "art_photos" --setting "creative"
```

If you want to use SlidesLib, add `--use_library`.

### Refinement
You will need the 1 st pass model outputs to start the refinement. 

To do refinement on a specific slide, you can provide the instruction, the existing slide, and the code used to generate the slide. The model will generate the refined code and save it to the refined_code_path. For example, you can do the refinement on the model output slide (in `slidesbench/examples/art_photos/slide_1/slide.png`) as follows:

```bash
cd refinement
python refine.py --existing_slide slidesbench/examples/art_photos/slide_1/slide.png --existing_code slidesbench/examples/art_photos/slide_1/original_code.py --instruction_path instruction.txt --refined_code_path refined_code.py --refinement_type fully_specified
```

You can also run refinement on all the slides generated by the `baseline` model by running:
```bash
cd refinement
python refine_all.py --base_path examples/art_photos/slide_1 --refinement_type fully_specified
```

Note: you need to have the `baseline` model outputs to start the refinement.

### Execution
To execute the python files generated by the `baseline` model, use the files under the script folder. For Llama or AutoPresent generated results, you might need to clean the code files first so that they are directly exectable.
```bash
cd scripts

# Clean up the code first if needed. You may need to modify this file for different baseline models. 
python clean_generated_code.py --base_dir "examples/" --target_file "Llama.py"

# Execute the python files.
python execute_each_file.py --base_dir "examples/" ----cleaned_file_name "cleaned_Llama.py"
```       
     
## AutoPresent: Training Slide Generation Models

To install SlidesLib, run:
```bash
pip install .
```
This will ensure that SlidesLib is installed.

### Model Weights
We release the Autopresent model weights for the three challenges:

[Detailed Instructions with Images](https://huggingface.co/JiaxinGe/llama-3.1-8b-autopresent_high_level)

[Detailed Instructions Only](https://huggingface.co/JiaxinGe/llama-3.1-8b-autopresent_detailed_instruction)

[High Level Instructions](https://huggingface.co/JiaxinGe/llama-3.1-8b-autopresent_high_level)

### Model Training

We provide the training code in the folder autopresent. 
In the train.py, set the "your-hf-token" to your own Huggingface token to push the trained model to your Huggingface account.
An example to run the code:
```bash
cd autopresent

# Train on fully specified task
python train.py --data_path "../slidesbench/dataset_train.csv" --model_name "meta-llama/Llama-3.1-8B-Instruct" --finetuned_model_name "Your_Huggingface_Account/Slide_llama_3.1_fully_specified"

# Train on visual absense task
python train.py --data_path "../slidesbench/dataset_train_no_img.csv" --model_name "meta-llama/Llama-3.1-8B-Instruct" --finetuned_model_name "Your_Huggingface_Account/Slide_llama_3.1_visual_absence"

# Train on creative generation task
python train.py --data_path "../slidesbench/dataset_train_high_level.csv" --model_name "meta-llama/Llama-3.1-8B-Instruct" --finetuned_model_name "Your_Huggingface_Account/Slide_llama_3.1_creative_generation"
```
Then, to use the trained model to generate the code on SlidesBench challenges, run:
```bash
cd autopresent

# Generate Code on fully specified task
python generate.py --path "path_to_your_trained_model" ----save_output_pth "presenter_fully_specified.py" --dataset_path "../slidesbench/dataset_test_fully_specified.csv"

# Generate Code on visual absence task
python generate.py --path "path_to_your_trained_model" ----save_output_pth "presenter_visual_absence.py" --dataset_path "../slidesbench/dataset_test_visual_absence.csv"

# Generate Code on creative generation task
python generate.py --path "path_to_your_trained_model" ----save_output_pth "presenter_creative_generation.py" --dataset_path "../slidesbench/dataset_test_high_level.csv"
```
For each slide deck, this will save a python file named save_output_pth under the according folder (eg. slidesbench/examples/art_photos/slide_1/presenter.py)

## Evaluation

We support two types of evaluation: reference-based evaluation and reference-free evaluation.

```bash
cd evaluate
python evaluate_all.py --slide_name "art_photos" # evaluate all pages in the slide deck
```

### Reference-based Evaluation

To evaluate how similar the model-generated slide is to the reference slide, to propose a set of metrics to regarding the spatial, textual, coloring similarities.

To evaluate a single page slide:

```bash
python page_eval.py \
--generated_pptx ../slidesbench/examples/art_photos/slide_1/gpt-4o-mini_model.pptx --generated_page 0 \
--reference_pptx ../slidesbench/examples/art_photos/art_photos.pptx --reference_page 0
```

To evaluate on the entire slide deck:

```bash
python slides_eval.py \
--example_dir ../slidesbench/examples/art_photos \
--predict_name "gpt-4o-mini_model"  # name of the pptx files
```

### Reference-free Evaluation

Independently to the the reference slide, we can also evaluate the generated slide based on varied design principles, including content, formatting, and overall presentation.

To evaluate a single page slide:

```bash
python reference_free_eval.py \
--image_path ../baseline/examples/art_photos/slide_1/gpt_4o_mini.jpg
```

## Citation
If you find our repo or paper useful, please cite us as:

```
@article{ge2025autopresent,
  title={AutoPresent: Designing Structured Visuals from Scratch},
  author={Ge, Jiaxin and Wang, Zora Zhiruo and Zhou, Xuhui and Peng, Yi-Hao and Subramanian, Sanjay and Tan, Qinyue and Sap, Maarten and Suhr, Alane and Fried, Daniel and Neubig, Graham and others},
  journal={arXiv preprint arXiv:2501.00912},
  year={2025}
}
```
